{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Bidirectional, LSTM, Dense\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import keras.callbacks\n",
    "import re\n",
    "import codecs\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bert_dir = r'C:\\Users\\rocker\\Python - deep learning\\Deep learning\\keras 大神\\bert'\n",
    "config_path = os.path.join(bert_dir, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(bert_dir, 'bert_model.ckpt')\n",
    "dict_path = os.path.join(bert_dir, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\rocker\\Python - deep learning\\Deep learning\\keras 大神\\content_df_0620.csv')\n",
    "data = data[data[\"status\"]==\"ok\"].drop([\"url\",\"context\",\"raw_content\",\"status\", \"content_status\"],axis = 1)\n",
    "data['aml_label'] = data['name'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data = data[data['aml_label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\rocker\\Python - deep learning\\Deep learning\\keras 大神\\content_df_0620.csv')\n",
    "data = data[data[\"status\"]==\"ok\"].drop([\"url\",\"context\",\"raw_content\",\"status\", \"content_status\"],axis = 1)\n",
    "data['aml_label'] = data['name'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data['name'] = data['name'].apply(lambda x: eval(x))\n",
    "data['content'] = data['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "                                                   .replace('報導', '＞')\n",
    "                                                   .replace('▲', '')\\\n",
    "                                                   .replace('。　', '。')\\\n",
    "                                                   .replace('\b', '')\\\n",
    "                                                   .replace('.', '')\\\n",
    "                                                   .replace(' ', '')\\\n",
    "                                                   .replace('“', '「')\\\n",
    "                                                   .replace('”', '」'))\n",
    "data['content'] = data['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: re.sub('「.{1,4}」', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: re.sub('｜.', '。', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2 = pd.read_csv(r'C:\\Users\\rocker\\extra data\\正式賽1~4.csv', encoding='cp950')\n",
    "#data2['name'] = data2['name'].apply(lambda x: eval(x))\n",
    "#data2['content'] = data2['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "#data2['content'] = data2['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "#                                                   .replace('報導', '＞')\n",
    "#                                                   .replace('▲', '')\\\n",
    "#                                                   .replace('。　', '。')\\\n",
    "#                                                   .replace('\b', '')\\\n",
    "#                                                   .replace('.', '')\\\n",
    "#                                                   .replace(' ', '')\\\n",
    "#                                                   .replace('“', '「')\\\n",
    "#                                                   .replace('”', '」'))\n",
    "#data2['content'] = data2['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))\n",
    "#data2['content'] = data2['content'].apply(lambda x: re.sub('「.{1,4}」', '', x))\n",
    "#data2['content'] = data2['content'].apply(lambda x: re.sub('｜.', '。', x))\n",
    "#data2 = data2[['news_id', 'name', 'content', 'aml_label']]\n",
    "#train2 = data2\n",
    "#train = train.append(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(r'C:\\Users\\rocker\\extra data\\test.csv')\n",
    "data2 = data2.drop(['title', 'link'], axis=1)\n",
    "data2['people_list'] = data2['people_list'].apply(lambda x: eval(x))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "data2['content'] = data2['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "                                                   .replace('報導', '＞')\n",
    "                                                   .replace('▲', '')\\\n",
    "                                                   .replace('。　', '。')\\\n",
    "                                                   .replace('\b', '')\\\n",
    "                                                   .replace('.', '')\\\n",
    "                                                   .replace(' ', '')\\\n",
    "                                                   .replace('“', '「')\\\n",
    "                                                   .replace('”', '」'))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('「.{1,4}」', '', x))\n",
    "data2.columns = ['news_id', 'content', 'aml_label', 'name']\n",
    "data2 = data2[['news_id', 'name', 'content', 'aml_label']]\n",
    "train2 = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(r'C:\\Users\\rocker\\extra data\\新增資料夾\\ltn_檢察官_法官_0719_過模型.csv', encoding='cp950')\n",
    "data2 = data2.drop(['title', 'link', 'people_list'], axis=1)\n",
    "data2['aml_label'] = data2['Name_prediction'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data2['news_id'] = range(1, len(data2)+1)\n",
    "data2['Name_prediction'] = data2['Name_prediction'].apply(lambda x: eval(x))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "data2['content'] = data2['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "                                                   .replace('報導', '＞')\n",
    "                                                   .replace('▲', '')\\\n",
    "                                                   .replace('。　', '。')\\\n",
    "                                                   .replace('\b', '')\\\n",
    "                                                   .replace('.', '')\\\n",
    "                                                   .replace(' ', '')\\\n",
    "                                                   .replace('“', '「')\\\n",
    "                                                   .replace('”', '」'))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('「.{1,4}」', '', x))\n",
    "train3 = data2\n",
    "train3.columns = ['news_id', 'content', 'name', 'aml_label']\n",
    "train3 = train3[['news_id', 'name', 'content', 'aml_label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 建立犯罪模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(dict_path):\n",
    "    \n",
    "    token_dict = {}\n",
    "    with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "        for line in reader:\n",
    "            token = line.strip()\n",
    "            token_dict[token] = len(token_dict)\n",
    "            \n",
    "    return token_dict\n",
    "\n",
    "def transfer(i):\n",
    "    \n",
    "    if i != 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def encoded(tokenizer, data, maxlen):\n",
    "    \n",
    "    x, y, z = [], [], []\n",
    "    if 'content' in data.columns:\n",
    "        for content in data['content']:\n",
    "            x1, x2 = tokenizer.encode(content, max_len=maxlen)\n",
    "            x3 = list(map(lambda x: 1 if x != 0 else 0, x1))\n",
    "            x.append(x1)\n",
    "            y.append(x2)\n",
    "            z.append(x3)\n",
    "    elif 'Sentence' in data.columns:\n",
    "        for content in data['Sentence']:\n",
    "            x1, x2 = tokenizer.encode(content, max_len=maxlen)\n",
    "            x3 = list(map(lambda x: 1 if x != 0 else 0, x1))\n",
    "            x.append(x1)\n",
    "            y.append(x2)\n",
    "            z.append(x3)\n",
    "            \n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "def rebuild_sentence(content, maxlen):\n",
    "    \n",
    "    if len(content) > maxlen:\n",
    "        return content[:round(maxlen/2)-1]+ '。' + content[len(content) - (maxlen - round(maxlen/2))+2:]\n",
    "    else:\n",
    "        return content\n",
    "\n",
    "\n",
    "def encoded_crime(tokenizer, data, maxlen):\n",
    "    \n",
    "    x, y, z = [], [], []\n",
    "    if 'content' in data.columns:\n",
    "        for content in data['content']:\n",
    "\n",
    "            content = rebuild_sentence(content, maxlen)\n",
    "            x1, x2 = tokenizer.encode(content, max_len=maxlen)\n",
    "            x3 = list(map(lambda x: 1 if x != 0 else 0, x1))\n",
    "            x.append(x1)\n",
    "            y.append(x2)\n",
    "            z.append(x3)\n",
    "    elif 'Sentence' in data.columns:\n",
    "        for content in data['Sentence']:\n",
    "            \n",
    "            content = rebuild_sentence(content, maxlen)    \n",
    "            x1, x2 = tokenizer.encode(content, max_len=maxlen)\n",
    "            x3 = list(map(lambda x: 1 if x != 0 else 0, x1))\n",
    "            x.append(x1)\n",
    "            y.append(x2)\n",
    "            z.append(x3)\n",
    "            \n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = create_tokenizer(dict_path)\n",
    "tokenizer = Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 512\n",
    "batch_size = 8\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rocker\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\rocker\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "def bert_LSTM_model():\n",
    "    \n",
    "    model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen)\n",
    "    sequence_output = model.layers[-9].output\n",
    "    sequence_output = Bidirectional(LSTM(128, return_sequences=False))(sequence_output)\n",
    "    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "    model = Model(model.input, output)\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "    model.layers[-2].trainable = True\n",
    "    return model\n",
    "\n",
    "model = bert_LSTM_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_1 = train[train.aml_label == 1]\n",
    "train0 = train[(~train.content.str.contains('檢察官|法官|殺人|傷人|酒駕|性侵|潑漆|大麻|逃逸|罷免|債|過失|警|毒')) & (train.aml_label == 0) & (~train.content.str.contains('罪')) & (~train.content.str.contains('詐欺|內線交易|背信|簽賭'))]\n",
    "crime_train = train1_1.append(train0)\n",
    "crime_train['news_id'] = range(1, len(crime_train)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.asarray(crime_train['aml_label'])\n",
    "input_id, segment_id, mask_input = encoded_crime(tokenizer, crime_train, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=train.shape[0],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(total_steps, warmup_steps, lr=1e-3, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = [\n",
    "                 keras.callbacks.EarlyStopping(monitor='val_acc', patience=1)\n",
    "                 #,ModelCheckpoint(filepath='AML_bert.h5', monitor='val_loss', save_best_only=True)\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/3\n",
      "2999/2999 [==============================] - 287s 96ms/step - loss: 0.0764 - acc: 0.9677\n",
      "Epoch 2/3\n",
      "2999/2999 [==============================] - 286s 95ms/step - loss: 0.0085 - acc: 0.9963\n",
      "Epoch 3/3\n",
      "2999/2999 [==============================] - 288s 96ms/step - loss: 0.0016 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b6029f0c88>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit([input_id, segment_id, mask_input],\n",
    "          label,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          #validation_split=0.1,\n",
    "          #callbacks=callback_list\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 建立 AML 犯罪模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_aml = 512\n",
    "batch_size = 8\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aml_train1_1 = train[train.aml_label == 1]\n",
    "aml_train1_2 = train2[train2.aml_label == 1]\n",
    "aml_train1_3 = train3[train3.aml_label == 1]\n",
    "\n",
    "aml_train2_1 = train[(train.content.str.contains('檢察官|法官|殺人|傷害|傷人|酒駕|性侵|潑漆|大麻|逃逸|罷免|債|過失|警|毒')) & (train.aml_label == 0) & (train.content.str.contains('罪')) & (~train.content.str.contains('洗錢|偽造|走私|貪污|侵占|販毒|賄|詐|內線交易|背信|香港|簽賭'))]\n",
    "aml_train2_2 = train2[(train2.content.str.contains('檢察官|法官|殺人|傷害|傷人|酒駕|性侵|潑漆|大麻|逃逸|罷免|債|過失|警|毒')) & (train2.aml_label == 0) & (train2.content.str.contains('罪')) & (~train2.content.str.contains('洗錢|偽造|走私|貪污|侵占|販毒|賄|詐|內線交易|背信|香港|簽賭'))]\n",
    "aml_train2_3 = train3[(train3.content.str.contains('檢察官|法官|殺人|傷害|傷人|酒駕|性侵|潑漆|大麻|逃逸|罷免|債|過失|警|毒')) & (train3.aml_label == 0) & (train3.content.str.contains('罪')) & (~train3.content.str.contains('洗錢|偽造|走私|貪污|侵占|販毒|賄|詐|內線交易|背信|香港|簽賭'))]\n",
    "\n",
    "aml_train3 = pd.read_csv(r'C:\\Users\\rocker\\extra data\\model1.5 data.csv',encoding='cp950')\n",
    "aml_train3 = aml_train3.drop(['title', 'link'], axis=1)\n",
    "aml_train3['name'] = '[]'\n",
    "aml_train3['name'] = aml_train3['name'].apply(lambda x: eval(x))\n",
    "\n",
    "aml_train = aml_train1_1.append(aml_train1_2).append(aml_train1_3).append(aml_train2_1).append(aml_train2_2).append(aml_train2_3).append(aml_train3)\n",
    "aml_train['news_id'] = range(1, len(aml_train)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.asarray(aml_train['aml_label'])\n",
    "input_id, segment_id, mask_input = encoded_crime(tokenizer, aml_train, maxlen_aml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_aml():\n",
    "    \n",
    "    model_aml = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen_aml)\n",
    "    sequence_output = model_aml.layers[-6].output\n",
    "    sequence_output = Dense(64, activation='relu')(sequence_output) \n",
    "    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "    model_aml = Model(model_aml.input, output)\n",
    "    return model_aml\n",
    "\n",
    "model_aml = model_aml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable = False\n",
    "for layer in model_aml.layers:\n",
    "    if layer.name == 'Encoder-12-MultiHeadSelfAttention':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=train.shape[0],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(total_steps, warmup_steps, lr=1e-3, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = [\n",
    "                 keras.callbacks.EarlyStopping(monitor='val_acc', patience=1)\n",
    "                 #,ModelCheckpoint(filepath='AML_bert.h5', monitor='val_loss', save_best_only=True)\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1546/1546 [==============================] - 49s 31ms/step - loss: 0.2960 - acc: 0.8803\n",
      "Epoch 2/4\n",
      "1546/1546 [==============================] - 48s 31ms/step - loss: 0.2021 - acc: 0.9263\n",
      "Epoch 3/4\n",
      "1546/1546 [==============================] - 47s 30ms/step - loss: 0.1345 - acc: 0.9450\n",
      "Epoch 4/4\n",
      "1546/1546 [==============================] - 47s 31ms/step - loss: 0.1046 - acc: 0.9664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b7f4b7dd08>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_aml.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_aml.fit([input_id, segment_id, mask_input],\n",
    "          label,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          #validation_split=0.1,\n",
    "          #callbacks=callback_list\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 建立 NER 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\rocker\\Python - deep learning\\Deep learning\\keras 大神\\content_df_0620.csv')\n",
    "data = data[data[\"status\"]==\"ok\"].drop([\"url\",\"context\",\"raw_content\",\"status\", \"content_status\"],axis = 1)\n",
    "data['aml_label'] = data['name'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data['name'] = data['name'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = data['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "                                                   .replace('報導', '＞')\n",
    "                                                   .replace('▲', '')\\\n",
    "                                                   .replace('。　', '。')\\\n",
    "                                                   .replace('\b', '')\\\n",
    "                                                   .replace('.', '')\\\n",
    "                                                   .replace(' ', '')\\\n",
    "                                                   .replace('“', '「')\\\n",
    "                                                   .replace('”', '」'))\n",
    "data['content'] = data['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把大於512的新聞以句點分段 (bert最多只能吃512)\n",
    "def split_content(data):\n",
    "    data_more_split = pd.DataFrame()\n",
    "    for i, row in data.iterrows():\n",
    "        if (len(row['content']) > 512) & (len(row['content']) <= 1024):\n",
    "\n",
    "            s = row['content']\n",
    "            s_split = [(i, abs(len(s)//2 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            idx_left = min(s_split, key=lambda x: x[1])[0]\n",
    "            first = \"。\".join([s_split[i][2] for i in range(idx_left)])\n",
    "            second = \"。\".join([s_split[i][2] for i in range(idx_left, len(s_split))])    \n",
    "            contents = [first, second]\n",
    "\n",
    "            for content in contents:\n",
    "                data_more_split = data_more_split.append(pd.DataFrame({'news_id':row['news_id'], 'content':content}, index=[66]), ignore_index=True)\n",
    "\n",
    "        elif len(row['content']) > 1024:\n",
    "\n",
    "            s = row['content']\n",
    "            s_split1 = [(i, abs(len(s)//3 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            s_split2 = [(i, abs(len(s)*2//3 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            idx_left1 = min(s_split1, key=lambda x: x[1])[0]\n",
    "            idx_left2 = min(s_split2, key=lambda x: x[1])[0]\n",
    "            first = \"。\".join([s_split1[i][2] for i in range(idx_left1)])\n",
    "            second = \"。\".join([s_split1[i][2] for i in range(idx_left1, idx_left2)])\n",
    "            third = \"。\".join([s_split1[i][2] for i in range(idx_left2, len(s_split1))])\n",
    "            contents = [first, second, third]\n",
    "\n",
    "            for content in contents:\n",
    "                data_more_split = data_more_split.append(pd.DataFrame({'news_id':row['news_id'], 'content':content}, index=[66]), ignore_index=True)\n",
    "    \n",
    "    return data_more_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_ner = 512\n",
    "batch_size = 8\n",
    "epochs = 3\n",
    "input_shape = (maxlen_ner, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分句\n",
    "train_ner = train.drop(['name', 'aml_label'], axis=1)\n",
    "data_less = train_ner[train_ner['content'].str.len() <= 512]\n",
    "data_more = train_ner[train_ner['content'].str.len() > 512]\n",
    "data_more_split = split_content(data_more)\n",
    "train_ner = data_less.append(data_more_split).reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3-1 用CKIP做NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得 NER input array (陳水扁貪汙 -> CKIP(陳水扁) -> 1 2 2 0 0)\n",
    "def transfer_NER(data, tokenizer, maxlen):\n",
    "    people_list = []\n",
    "    first_name = ['申', '龔', '馮', '昌', '劉', '習', '陽', '顧', '鍾', '胡', '許', '魏', '傅', '季', '扶', '柳', '狄', '焦', '封', '李', '羿', '刁', '和', '邴',    '陸', '王', '杜', '能', '侯', '伍', '平', '竺', '樂', '繆', '欒', '湛',    '道', '花', '賴', '浦', '萬', '章', '宮', '勾', '邵', '印', '夏', '杭',    '溥', '左', '池', '公', '閻', '符', '奚', '臧', '羅', '空', '璩', '巴',    '酈', '范', '談', '金', '顏', '慎', '郭', '僪', '聞', '車', '闞', '相',    '童', '雙', '方', '莊', '容', '姚', '田', '薛', '閔', '翟', '簡',    '蔚', '茹', '淩', '戴', '余', '鞏', '房', '富', '牛', '饒', '計', '居',    '後', '舒', '席', '翁', '祝', '鬱', '訾', '隆', '匡', '弘', '曆', '範',    '越', '趙', '卻', '岑', '隗', '冷', '張', '山', '松', '柯', '嵇', '韓',    '蕭', '褚', '殳', '滕', '滿', '洪', '荀', '庾', '廖', '盧', '危', '竇',    '曾', '郎', '遊', '穀', '慕', '禹', '凌', '廉', '邢', '梁', '葉',    '郝', '終', '齊', '藺', '曹', '全', '高', '樊', '史', '桂', '廣', '段',    '江', '餘', '袁', '弓', '牧', '魚', '儲', '尚', '逄', '尹', '通', '懷',    '皮', '何', '倪', '包', '晁', '涂', '蓬', '屠', '巫', '須', '巢', '卞',    '楊', '成', '孟', '楚', '呂', '古', '毋', '伊', '賁', '喻', '糜',    '蔔', '艾', '藍', '龐', '諸', '別', '任', '管', '冀', '壽', '惠', '梅',    '孫', '從', '康', '常', '駱', '鞠', '沈', '黨', '沙', '鳳', '郁', '邊',    '仰', '溫', '路', '逮', '賀', '雷', '鈄', '明', '裴', '滑', '毛', '費',    '關', '時', '步', '麴', '裘', '蒲', '司', '查', '錢', '盛', '霍', '鮑',    '彭', '龍', '沃', '單', '勞', '秋', '祖', '殷', '茅', '敖', '郗', '石',    '鐘', '嚴', '畢', '燕', '姜', '經', '程', '厙', '柏', '汪', '婁', '胥',    '聶', '邰', '桑', '辛', '扈', '穆', '仲', '紅', '項', '師', '桓', '黃',    '堵', '貢', '詹', '朱', '蔡', '戈', '于', '甄', '束', '屈', '索', '晏',    '阮', '魯', '虞', '歐', '濮', '俞', '黎', '文', '應', '姬', '貝', '籍',    '莘', '戚', '鄭', '郜', '景', '宋', '宗', '昝', '卓', '蒯', '馬', '顔',    '蘇', '衛', '東', '瞿', '蒼', '莫', '邱', '潘', '家', '林', '芮', '麻',    '元', '武', '強', '鈕', '陳', '井', '於', '游', '耿', '柴', '荊', '韶',    '易', '宿', '施', '鹹', '秦', '班', '甯', '汲', '酆', '暴', '尤',    '祿', '苗', '權', '仇', '都', '羊', '榮', '陶', '支', '賈', '白', '葛',    '暨', '解', '靳', '伏', '唐', '華', '吉', '融', '豐', '安', '衡', '那',    '闕', '俄', '盍', '鄔', '蒙', '利', '鄂', '謝', '宓', '湯', '喬', '孔',    '養', '紀', '幹', '牟', '連', '宰', '蔣', '雍', '益', '寇', '祁', '熊',    '崔', '丁', '薊', '譚', '吳', '烏', '周', '農', '徐', '充', '向', '宦',    '董', '甘', '冉', '韋', '米', '鄒', '鄧', '戎', '水']\n",
    "    label = np.zeros([len(data), maxlen])\n",
    "    \n",
    "    # 用CKIP抓出每個新聞的名字\n",
    "    for index, (_, row) in enumerate(data.iterrows()):\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            print(index)\n",
    "        \n",
    "        token = tokenizer.tokenize(row['content'][0:maxlen])\n",
    "\n",
    "        if len(token) > maxlen:\n",
    "            token = token[0:maxlen-1]\n",
    "            token.append('[SEP]')\n",
    "\n",
    "        y = np.zeros([maxlen])\n",
    "        content = ''.join(token)\n",
    "        \n",
    "        #CKIP\n",
    "        word_sentence_list = ws([content],\n",
    "                    sentence_segmentation=True,\n",
    "                    segment_delimiter_set={'?', '？', '!', '！', '。', ',', '，', ';', ':', '、'})\n",
    "        pos_sentence_list = pos(word_sentence_list)\n",
    "        entity_sentence_list = ner(word_sentence_list, pos_sentence_list)\n",
    "        \n",
    "        people = [people for people in list(entity_sentence_list[0]) if (people[2] == 'PERSON') & (people[1] < maxlen)]\n",
    "        people = [people for people in people if (len(people[3]) < 5) | ((len(people[3]) >= 5) & ('#' not in people[3]))]\n",
    "        people.sort()\n",
    "        people_list.append(people)\n",
    "        \n",
    "        #轉換成input array\n",
    "        j = 0\n",
    "        for person in people: \n",
    "            for i, _ in enumerate(token):        \n",
    "                if token[i:i+len(person[3])] == list(person[3]):\n",
    "                    if len(person) == 1:\n",
    "                        y[i+j] = 1\n",
    "                        token = token[i+1:]\n",
    "                        j = i+j+1\n",
    "                        break\n",
    "\n",
    "                    y[i+j] = 1\n",
    "                    y[i+j+1:i+j+len(person[3])] = 2\n",
    "                    token = token[i+len(person[3]):]\n",
    "                    j = i+j+len(person[3])\n",
    "                    break\n",
    "        label[index, :] = y\n",
    "    \n",
    "    #用不到 people_list，只是檢查用\n",
    "    return people_list, label.reshape([label.shape[0], label.shape[1], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_list, label = transfer_NER(train_ner, tokenizer, maxlen=maxlen_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_list = pd.to_csv(r'C:\\Users\\rocker\\ner.csv',encoding='utf_8_sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 用人工辨識做NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得 NER input array (陳水扁貪汙 -> CKIP(陳水扁) -> 1 2 2 0 0)\n",
    "def transfer_NER_array(data, people_list, tokenizer, maxlen):\n",
    "    \n",
    "    label = np.zeros([len(data), maxlen])\n",
    "    \n",
    "    # 用CKIP抓出每個新聞的名字\n",
    "    for (index, (_, row)), people in zip(enumerate(data.iterrows()), people_list):\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            print(index)\n",
    "        \n",
    "        token = tokenizer.tokenize(row['content'][0:maxlen])\n",
    "\n",
    "        if len(token) > maxlen:\n",
    "            token = token[0:maxlen-1]\n",
    "            token.append('[SEP]')\n",
    "\n",
    "        y = np.zeros([maxlen])\n",
    "        content = ''.join(token)\n",
    "        \n",
    "        #轉換成input array\n",
    "        j = 0\n",
    "        for person in people: \n",
    "            for i, _ in enumerate(token):        \n",
    "                if token[i:i+len(person[3])] == list(person[3]):\n",
    "                    if len(person) == 1:\n",
    "                        y[i+j] = 1\n",
    "                        token = token[i+1:]\n",
    "                        j = i+j+1\n",
    "                        break\n",
    "\n",
    "                    y[i+j] = 1\n",
    "                    y[i+j+1:i+j+len(person[3])] = 2\n",
    "                    token = token[i+len(person[3]):]\n",
    "                    j = i+j+len(person[3])\n",
    "                    break\n",
    "        label[index, :] = y\n",
    "    \n",
    "    #用不到 people_list，只是檢查用\n",
    "    return label.reshape([label.shape[0], label.shape[1], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "people = pd.read_csv(r'C:\\Users\\rocker\\ner.csv',encoding='big5hkscs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_list = list(people['people_list'].apply(lambda x: eval(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n"
     ]
    }
   ],
   "source": [
    "label = transfer_NER_array(train_ner, people_list, tokenizer, maxlen=maxlen_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id, segment_id, mask_input = encoded(tokenizer, train_ner, maxlen=maxlen_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_BiLSTM_CRF_model():\n",
    "    \n",
    "    ner_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen_ner)\n",
    "    bert_output = ner_model.layers[-9].output\n",
    "    X = Lambda(lambda x: x[:, 0: input_shape[0]])(bert_output)\n",
    "    X = Bidirectional(LSTM(128, return_sequences=True))(X)\n",
    "    output = CRF(3, sparse_target = True)(X)    \n",
    "    ner_model = Model(ner_model.input, output)\n",
    "    \n",
    "    for layer in ner_model.layers:\n",
    "        layer.trainable = False\n",
    "    ner_model.layers[-1].trainable = True\n",
    "    ner_model.layers[-2].trainable = True\n",
    "    \n",
    "    return ner_model\n",
    "\n",
    "ner_model = bert_BiLSTM_CRF_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=data.shape[0],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(total_steps, warmup_steps, lr=1e-3, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = [\n",
    "                 keras.callbacks.EarlyStopping(monitor='val_crf_accuracy', patience=1)\n",
    "                 #,ModelCheckpoint(filepath='AML_bert.h5', monitor='val_loss', save_best_only=True)\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "9256/9256 [==============================] - 1100s 119ms/step - loss: 0.0201 - crf_accuracy: 0.9958\n",
      "Epoch 2/3\n",
      "9256/9256 [==============================] - 1088s 118ms/step - loss: 0.0080 - crf_accuracy: 0.9978\n",
      "Epoch 3/3\n",
      "9256/9256 [==============================] - 1087s 117ms/step - loss: 0.0077 - crf_accuracy: 0.9979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b80f7dd588>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model.compile(optimizer=optimizer,\n",
    "                  loss=crf_loss,\n",
    "                  metrics=[crf_accuracy])\n",
    "\n",
    "ner_model.fit([input_id, segment_id, mask_input],\n",
    "          label,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size)\n",
    "          #validation_split=0.1,\n",
    "          #callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 建立人名 AML 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\rocker\\Python - deep learning\\Deep learning\\keras 大神\\content_df_0620.csv')\n",
    "data = data[data[\"status\"]==\"ok\"].drop([\"url\",\"context\",\"raw_content\",\"status\", \"content_status\"],axis = 1)\n",
    "data['aml_label'] = data['name'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data = data[data['aml_label'] == 1]\n",
    "\n",
    "data = pd.read_csv(r'C:\\Users\\rocker\\Python - deep learning\\Deep learning\\keras 大神\\content_df_0620.csv')\n",
    "data = data[data[\"status\"]==\"ok\"].drop([\"url\",\"context\",\"raw_content\",\"status\", \"content_status\"],axis = 1)\n",
    "data['aml_label'] = data['name'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data['name'] = data['name'].apply(lambda x: eval(x))\n",
    "data['content'] = data['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "                                                   .replace('報導', '＞')\n",
    "                                                   .replace('▲', '')\\\n",
    "                                                   .replace('。　', '。')\\\n",
    "                                                   .replace('\b', '')\\\n",
    "                                                   .replace('.', '')\\\n",
    "                                                   .replace(' ', '')\\\n",
    "                                                   .replace('“', '「')\\\n",
    "                                                   .replace('”', '」'))\n",
    "data['content'] = data['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: re.sub('「.{1,4}」', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: re.sub('｜.', '。', x))\n",
    "\n",
    "train = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['aml_label'] == 1]\n",
    "train['news_id'] = range(1, len(train)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_sentences = 256\n",
    "batch_size = 8\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把大於512的新聞以句點分段 (bert最多只能吃512)\n",
    "def split_content_model2(data):\n",
    "    data_more_split = pd.DataFrame()\n",
    "    for i, row in data.iterrows():\n",
    "        if (len(row['content']) > 512) & (len(row['content']) <= 1024):\n",
    "\n",
    "            s = row['content']\n",
    "            s_split = [(i, abs(len(s)//2 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            idx_left = min(s_split, key=lambda x: x[1])[0]\n",
    "            first = \"。\".join([s_split[i][2] for i in range(idx_left)])\n",
    "            second = \"。\".join([s_split[i][2] for i in range(idx_left, len(s_split))])    \n",
    "            contents = [first, second]\n",
    "\n",
    "            for content in contents:\n",
    "                data_more_split = data_more_split.append(pd.DataFrame({'news_id':row['news_id'], 'content':content, 'aml_label':row['aml_label']}, index=[66]), ignore_index=True)\n",
    "\n",
    "        elif len(row['content']) > 1024:\n",
    "\n",
    "            s = row['content']\n",
    "            s_split1 = [(i, abs(len(s)//3 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            s_split2 = [(i, abs(len(s)*2//3 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            idx_left1 = min(s_split1, key=lambda x: x[1])[0]\n",
    "            idx_left2 = min(s_split2, key=lambda x: x[1])[0]\n",
    "            first = \"。\".join([s_split1[i][2] for i in range(idx_left1)])\n",
    "            second = \"。\".join([s_split1[i][2] for i in range(idx_left1, idx_left2)])\n",
    "            third = \"。\".join([s_split1[i][2] for i in range(idx_left2, len(s_split1))])\n",
    "            contents = [first, second, third]\n",
    "\n",
    "            for content in contents:\n",
    "                data_more_split = data_more_split.append(pd.DataFrame({'news_id':row['news_id'], 'content':content, 'aml_label':row['aml_label']}, index=[66]), ignore_index=True)\n",
    "    \n",
    "    return data_more_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentences_model2(test, people_list, stick, tokenizer=tokenizer, maxlen=maxlen_sentences):\n",
    "    \n",
    "    AML = pd.DataFrame(columns=['news_id', 'Name', 'Sentence'])\n",
    "    aml_highrisk = np.asarray(test['content'])\n",
    "    news_ids = np.asarray(test['news_id'])\n",
    "    people_list = list(test['people_list'])\n",
    "    \n",
    "    for k, (news_id, y_news) in enumerate(zip(news_ids ,aml_highrisk)): \n",
    "        #用，。？切分句子\n",
    "        y_news = y_news.replace('。','=。')\n",
    "        y_news = y_news.replace('，','*，')\n",
    "        y_news = y_news.replace('？','+？')\n",
    "        y_news = y_news.replace('；','{；')\n",
    "        \n",
    "        news = re.split('，|。|？|；', y_news)\n",
    "        \n",
    "        news = [news.replace('=','。') for news in news]\n",
    "        news = [news.replace('*','，') for news in news]\n",
    "        news = [news.replace('+','？') for news in news]\n",
    "        news = [news.replace('{','；') for news in news]\n",
    "#############################################################################################################\n",
    "        for i in range(len(people_list[k])):\n",
    "            # 找出人名存在的 news index\n",
    "            index = [index for index, _ in enumerate(news) if people_list[k][i] in _]                                \n",
    "            x = [people for people in people_list[k] if people_list[k][i] not in people]\n",
    "\n",
    "            name = [name for name in people_list[k] if name != people_list[k][i]]\n",
    "            name = [name for name in name if ((len(name)<3) & (name[0] != people_list[k][i][0])) | (len(name)>=3)]\n",
    "            name.sort(reverse=True)\n",
    "\n",
    "            for j in index:\n",
    "\n",
    "                mid = news[j]\n",
    "                \n",
    "                if name != []:\n",
    "                    name = [name.replace('?', '\\?') for name in name]\n",
    "                    mid = re.sub('|'.join(name), '', mid)\n",
    "                \n",
    "                if j == 0:\n",
    "                    if len(news) != 1:\n",
    "                        end = news[j+1]  \n",
    "\n",
    "                        if True in [people in news[j+1] for people in x]:\n",
    "                            sentences = mid\n",
    "                        else:\n",
    "                            sentences = mid + end\n",
    "                    elif len(news) == 1:\n",
    "                        sentences = mid\n",
    "\n",
    "                elif j+1 == len(news):\n",
    "                    start = news[j-1]\n",
    "                    if '。' in start:\n",
    "                        start = ''\n",
    "\n",
    "                    if True in [people in news[j-1] for people in x]:\n",
    "                        sentences = mid\n",
    "                    else:\n",
    "                        sentences = start + mid\n",
    "\n",
    "                else:\n",
    "                    end = news[j+1]\n",
    "                    start = news[j-1]                    \n",
    "\n",
    "                    if '。' in start:\n",
    "                        start = ''\n",
    "                    \n",
    "                    if '。' in mid:\n",
    "                        end = ''                 \n",
    "\n",
    "                    if (True not in [people in news[j-1] for people in x]) & (True not in [people in news[j+1] for people in x]):\n",
    "                        sentences = start + mid + end\n",
    "                    elif (True not in [people in news[j-1] for people in x]) & (True in [people in news[j+1] for people in x]):\n",
    "                        sentences = start + mid\n",
    "                    elif (True in [people in news[j-1] for people in x]) & (True not in [people in news[j+1] for people in x]):\n",
    "                        sentences = mid + end\n",
    "                    else:\n",
    "                        sentences = mid  \n",
    "   \n",
    "                AML = AML.append(pd.DataFrame([[news_ids[k] ,people_list[k][i], sentences]], columns=AML.columns))\n",
    "                #break  \n",
    "    \n",
    "    \n",
    "    # 把指向同一人的姓名改成一樣（陳男 -> 陳水扁），若指向多人則不改（陳男 -> 陳致中、陳水扁）\n",
    "    # 將預測不完整的名字回填（王音 -> 王音之）\n",
    "    name_list = []\n",
    "    for ids in AML['news_id'].unique():\n",
    "        \n",
    "        full_name = [name for name in AML[(AML['news_id'] == ids)]['Name']]\n",
    "        full_3name = list(set([name for name in AML[(AML['news_id'] == ids)]['Name'] if len(name) in [3, 4]]))\n",
    "        full_longname = list(set([name for name in AML[(AML['news_id'] == ids)]['Name'] if len(name) > 3]))\n",
    "        \n",
    "        a = Counter([name[0] for name in full_3name])\n",
    "        keep = [k for k,v in a.items() if v == 1]\n",
    "\n",
    "        name_dict_2 = dict(zip([name[0:2] for name in full_3name], full_3name))  # ex: {'王音': '王音之'}\n",
    "        name_dict_3 = dict(zip([name[1:] for name in full_longname], full_longname))\n",
    "        name_dict_4 = dict(zip([name[:2] for name in full_longname], full_longname))\n",
    "        \n",
    "        for name in full_name:\n",
    "            if (name in name_dict_2.keys()) & (len(name) == 2):\n",
    "                name_list.append(name_dict_2.get(name))\n",
    "            elif (name in name_dict_3.keys()):\n",
    "                name_list.append(name_dict_3.get(name))\n",
    "            elif (name in name_dict_4.keys()):\n",
    "                name_list.append(name_dict_4.get(name))            \n",
    "            else:\n",
    "                name_list.append(name)\n",
    "                \n",
    "    \n",
    "    # 排除重複資料、排除一字、兩字簡稱、兩字三字四字姓不在姓氏表中的人\n",
    "    AML['Name'] = name_list\n",
    "    AML = AML.drop_duplicates()\n",
    "    AML = AML[AML['Name'].apply(lambda x: (len(x) > 1) )]\n",
    "    AML = AML[~AML['Name'].apply(lambda x: (len(x) == 2) & (x[1] in ['男', '嫌', '婦', '夫', '某', '女', '妻',\\\n",
    "                   \n",
    "                                                                     '員', '稱', '家', '哥', '媽', '生', \\\n",
    "                                                                     '揆', '董', '母', '公', '少', '翁',\\\n",
    "                                                                     '粉', '仔', '氏', '父','童', '弟', '嬤','姊','姐','妹', '警']))]\n",
    "    AML = AML[~AML['Name'].apply(lambda x: (len(x) == 2) & (x[0] in ['老', '小', '阿']))]\n",
    "    AML = AML[~AML['Name'].apply(lambda x: (len(x) == 2) & (x[1] == x[0]))]\n",
    "    return AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得名字 (預測結果為onehot的狀態)\n",
    "def get_name(input_id, y_pred):\n",
    "    \n",
    "    label_list = []\n",
    "    word_dict = {v: k for k, v in token_dict.items()}\n",
    "    \n",
    "    for input_data, y in zip(input_id, y_pred):\n",
    "        people_index = ''.join([str(a) for a in list(y)])\n",
    "        j = 0\n",
    "        name_list = []\n",
    "        split_index = re.findall('[12]2*', people_index)\n",
    "        name = ''.join([word_dict.get(input_data[index]) for index, value in enumerate(y) if value != 0])\n",
    "        \n",
    "        # [UNK], [PAD]會被算成 5 個字元，避免轉換成文字的index因長度不同對不上，故用 1 個字元的其他符號替代\n",
    "        # 王春甡 -> 王春[UNK] -> 王春?\n",
    "        name = name.replace('[UNK]','?')\n",
    "        name = name.replace('[PAD]','!')\n",
    "        \n",
    "        for i in split_index:\n",
    "            name_list.append(name[0+j:len(i)+j])\n",
    "            j = len(i) + j\n",
    "            \n",
    "        name_list = [name for name in name_list]\n",
    "        label_list.append(list(set(name_list)))\n",
    "    \n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 將超過 512 的句子以句點拆成多句分段預測\n",
    "train_ner = train.drop(['name'], axis=1)\n",
    "data_less = train_ner[train_ner['content'].str.len() <= 512]\n",
    "data_more = train_ner[train_ner['content'].str.len() > 512]\n",
    "data_more_split = split_content_model2(data_more)\n",
    "train_ner = data_less.append(data_more_split).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "# 3. NER 預測人名\n",
    "input_id, segment_id, mask_input = encoded(tokenizer, train_ner, maxlen=maxlen_ner)\n",
    "prediction = ner_model.predict([input_id, segment_id, mask_input])\n",
    "y_pred = np.argmax(prediction, axis=-1)\n",
    "people_list = get_name(input_id, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2. 將超過 512 的句子以句點拆成多句分段預測\n",
    "train_ner = train.drop(['name'], axis=1)\n",
    "data_less = train_ner[train_ner['content'].str.len() <= 512]\n",
    "data_more = train_ner[train_ner['content'].str.len() > 512]\n",
    "data_more_split = split_content_model2(data_more)\n",
    "train_ner = data_less.append(data_more_split).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "# 3. NER 預測人名\n",
    "input_id, segment_id, mask_input = encoded(tokenizer, train_ner, maxlen=maxlen_ner)\n",
    "prediction = ner_model.predict([input_id, segment_id, mask_input])\n",
    "y_pred = np.argmax(prediction, axis=-1)\n",
    "people_list = get_name(input_id, y_pred)\n",
    "\n",
    "\n",
    "# 4. 將拆開的句子組合回去\n",
    "train_ner['people_list'] = people_list\n",
    "content = train_ner[['news_id', 'content', 'aml_label']]\n",
    "content = content.groupby(['news_id', 'aml_label'])['content'].apply(lambda x : '。'.join(x)).reset_index()\n",
    "people = train_ner[['news_id', 'aml_label', 'people_list']]\n",
    "people = people.groupby(['news_id', 'aml_label'])['people_list'].agg(sum).reset_index()\n",
    "people['people_list'] = [list(set(people)) for people in people['people_list']]\n",
    "train_ner = pd.merge(content, people, on=['news_id', 'aml_label'], how='left')\n",
    "\n",
    "# 5. 將 [UNK], [PAD] 轉換回來 (王春? -> 王春甡)\n",
    "for _, row in train_ner.iterrows():\n",
    "    for i, name in enumerate(row['people_list']):\n",
    "        if ('?' in name) | ('!' in name):\n",
    "            reexp = name.replace('?', '.').replace('!', '.')\n",
    "            reexp = re.compile(reexp,re.IGNORECASE)\n",
    "            row['people_list'][i] = re.search(reexp, row['content']).group()\n",
    "            \n",
    "AML = predict_sentences_model2(train_ner, list(train_ner['people_list']), tokenizer=tokenizer, maxlen=maxlen_sentences, stick=False)\n",
    "\n",
    "#合併句子\n",
    "AML = AML.groupby(['news_id', 'Name'])['Sentence'].apply('，'.join).reset_index()\n",
    "\n",
    "train_name = train.drop(['content'], axis=1)\n",
    "s = train_name.apply(lambda x: pd.Series(x['name']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = 'Name'\n",
    "train_name = train_name.drop('name', axis=1).join(s)\n",
    "\n",
    "aml_train = pd.merge(AML, train_name, on=['news_id', 'Name'], how='left')\n",
    "aml_train = aml_train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1058"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aml_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aml_train[aml_train['aml_label'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.asarray(aml_train['aml_label'])\n",
    "input_id, segment_id, mask_input = encoded(tokenizer, aml_train, maxlen_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_model():\n",
    "    \n",
    "    model2 = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen_sentences)\n",
    "    sequence_output = model2.layers[-6].output\n",
    "    sequence_output = Dense(16, activation='relu')(sequence_output)\n",
    "    sequence_output = Dense(16, activation='relu')(sequence_output)\n",
    "    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "    model2 = Model(model2.input, output)\n",
    "\n",
    "    return model2\n",
    "\n",
    "model2 = bert_model()\n",
    "\n",
    "set_trainable = False\n",
    "for layer in model2.layers:\n",
    "    if layer.name == 'Encoder-12-MultiHeadSelfAttention':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=train.shape[0],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(total_steps, warmup_steps, lr=1e-3, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = [\n",
    "                 keras.callbacks.EarlyStopping(monitor='val_acc', patience=1)\n",
    "                 #,ModelCheckpoint(filepath='AML_bert.h5', monitor='val_loss', save_best_only=True)\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1058/1058 [==============================] - 17s 16ms/step - loss: 0.4349 - acc: 0.8129\n",
      "Epoch 2/3\n",
      "1058/1058 [==============================] - 15s 14ms/step - loss: 0.2830 - acc: 0.8904\n",
      "Epoch 3/3\n",
      "1058/1058 [==============================] - 14s 14ms/step - loss: 0.2711 - acc: 0.9008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b80419efc8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'])\n",
    "\n",
    "model2.fit([input_id, segment_id, mask_input],\n",
    "          label,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          #validation_split=0.1,\n",
    "          #callbacks=callback_list\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model1_0801官方.h5')\n",
    "model_aml.save_weights('model1.5_0801官方.h5')\n",
    "model2.save_weights('model2_0801官方.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
