{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.callbacks\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Bidirectional, LSTM, Dense\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from datetime import datetime\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 變數區"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'C:\\Users\\jason\\Desktop\\python\\01_project\\01_esun\\api\\data'\n",
    "config_path = os.path.join(model_path, r'bert\\bert_config.json')\n",
    "checkpoint_path = os.path.join(model_path, r'bert\\bert_model.ckpt')\n",
    "dict_path = os.path.join(model_path, r'bert\\vocab.txt')\n",
    "bert_LSTM_model_path = os.path.join(model_path, r'model1.h5')\n",
    "bert_LSTM_model_path_2 = os.path.join(model_path, r'model1_5.h5')\n",
    "ner_model_path = os.path.join(model_path, r'ner_model.h5')\n",
    "aml_model_2_path = os.path.join(model_path, r'model2.h5')\n",
    "maxlen = 512\n",
    "maxlen_aml = 512\n",
    "maxlen_ner = 512\n",
    "input_shape = (maxlen_ner, )\n",
    "maxlen_sentences = 256\n",
    "aml_threshold = 0.4\n",
    "threshold = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create model & load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "# model 1\n",
    "def bert_LSTM_model():\n",
    "    model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen)\n",
    "    sequence_output = model.layers[-9].output\n",
    "    sequence_output = Bidirectional(LSTM(128, return_sequences=False))(sequence_output)\n",
    "    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "    model = Model(model.input, output)\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "    model.layers[-2].trainable = True\n",
    "    return model\n",
    "\n",
    "# model 1.5\n",
    "def bert_model_1_5():\n",
    "    model1_5 = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen_aml)\n",
    "    sequence_output = model1_5.layers[-6].output\n",
    "    sequence_output = Dense(64, activation='relu')(sequence_output)\n",
    "    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "    model1_5 = Model(model1_5.input, output)\n",
    "    return model1_5\n",
    "\n",
    "# ner model\n",
    "def bert_BiLSTM_CRF_model():\n",
    "    ner_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen_ner)\n",
    "    bert_output = ner_model.layers[-9].output\n",
    "    X = Lambda(lambda x: x[:, 0: input_shape[0]])(bert_output)\n",
    "    X = Bidirectional(LSTM(128, return_sequences=True))(X)\n",
    "    output = CRF(3, sparse_target = True)(X)    \n",
    "    ner_model = Model(ner_model.input, output)\n",
    "    \n",
    "    for layer in ner_model.layers:\n",
    "        layer.trainable = False\n",
    "    ner_model.layers[-1].trainable = True\n",
    "    ner_model.layers[-2].trainable = True\n",
    "    \n",
    "    return ner_model\n",
    "    \n",
    "# model 2\n",
    "def bert_model():\n",
    "    model2 = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen_sentences)\n",
    "    sequence_output = model2.layers[-6].output\n",
    "    sequence_output = Dense(64, activation='relu')(sequence_output)\n",
    "    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "    model2 = Model(model2.input, output)\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jason\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\jason\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = bert_LSTM_model()\n",
    "model.load_weights(bert_LSTM_model_path)\n",
    "\n",
    "model1_5 = bert_model_1_5()\n",
    "model1_5.load_weights(bert_LSTM_model_path_2)\n",
    "\n",
    "ner_model = bert_BiLSTM_CRF_model()\n",
    "ner_model.load_weights(ner_model_path)\n",
    "\n",
    "model2 = bert_model()\n",
    "model2.load_weights(aml_model_2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleansing\n",
    "def clean_marks(content):\n",
    "    content = re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', content)\n",
    "    content = content.strip() \\\n",
    "                     .replace('記者', '＜') \\\n",
    "                     .replace('報導', '＞') \\\n",
    "                     .replace('▲', '') \\\n",
    "                     .replace('。　', '。') \\\n",
    "                     .replace('\b', '') \\\n",
    "                     .replace('.', '') \\\n",
    "                     .replace(' ', '') \\\n",
    "                     .replace('“', '「') \\\n",
    "                     .replace('”', '」')\n",
    "    content = re.sub('＜[^＞]*＞', '', content)\n",
    "    content = re.sub('「.{1,4}」', '', content)\n",
    "    content = re.sub('｜.', '。', content)\n",
    "\n",
    "    return content\n",
    "\n",
    "# data encoded \n",
    "def create_tokenizer(dict_path):\n",
    "    \n",
    "    token_dict = {}\n",
    "    with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "        for line in reader:\n",
    "            token = line.strip()\n",
    "            token_dict[token] = len(token_dict)\n",
    "    tokenizer = Tokenizer(token_dict)\n",
    "    \n",
    "    return tokenizer, token_dict\n",
    "\n",
    "def encoded(tokenizer, data, maxlen):\n",
    "    \n",
    "    x, y, z = [], [], []\n",
    "    x1, x2 = tokenizer.encode(data, max_len=maxlen)\n",
    "    x3 = list(map(lambda x: 1 if x != 0 else 0, x1))\n",
    "    x.append(x1)\n",
    "    y.append(x2)\n",
    "    z.append(x3)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z = np.array(z)\n",
    "    data = [x, y, z]\n",
    "\n",
    "    return data\n",
    "\n",
    "def encoded_2(tokenizer, data, maxlen):\n",
    "\n",
    "    x, y, z = [], [], []\n",
    "    for content in data['sentence']:\n",
    "        x1, x2 = tokenizer.encode(content, max_len=maxlen)\n",
    "        x3 = list(map(lambda x: 1 if x != 0 else 0, x1))\n",
    "        x.append(x1)\n",
    "        y.append(x2)\n",
    "        z.append(x3)\n",
    "\n",
    "    return x, y, z\n",
    "\n",
    "def rebuild_sentence(content, maxlen):\n",
    "    \n",
    "    if len(content) > maxlen:\n",
    "        return content[:round(maxlen/2)-1]+ '。' + content[len(content) - (maxlen - round(maxlen/2))+2:]\n",
    "    else:\n",
    "        return content\n",
    "\n",
    "def split_content(content):\n",
    "\n",
    "    if (len(content) > 512) & (len(content) <= 1024):\n",
    "\n",
    "        s_split = [(i, abs(len(content)//2 - content.find(x)), x) for i, x in enumerate(content.split('。'))]\n",
    "        idx_left = min(s_split, key=lambda x: x[1])[0]\n",
    "        first = \"。\".join([s_split[i][2] for i in range(idx_left)])\n",
    "        second = \"。\".join([s_split[i][2] for i in range(idx_left, len(s_split))])    \n",
    "        contents = [first, second]\n",
    "        \n",
    "        return contents\n",
    "\n",
    "    elif len(content) > 1024:\n",
    "\n",
    "        s_split1 = [(i, abs(len(content)//3 - content.find(x)), x) for i, x in enumerate(content.split('。'))]\n",
    "        s_split2 = [(i, abs(len(content)*2//3 - content.find(x)), x) for i, x in enumerate(content.split('。'))]\n",
    "        idx_left1 = min(s_split1, key=lambda x: x[1])[0]\n",
    "        idx_left2 = min(s_split2, key=lambda x: x[1])[0]\n",
    "        first = \"。\".join([s_split1[i][2] for i in range(idx_left1)])\n",
    "        second = \"。\".join([s_split1[i][2] for i in range(idx_left1, idx_left2)])\n",
    "        third = \"。\".join([s_split1[i][2] for i in range(idx_left2, len(s_split1))])\n",
    "        contents = [first, second, third]\n",
    "        \n",
    "        return contents\n",
    "    \n",
    "    else:\n",
    "        return [content]\n",
    "\n",
    "# feature engineering\n",
    "# da pattern\n",
    "def search_da(name: str, sentence: str) -> bool:\n",
    "    '''\n",
    "    搜索pattern: \"(職業)(人名)(指定動作)\" \n",
    "    可允許兩人姓名中間有、，再多就不行了。\n",
    "    '''\n",
    "    da_list = ['說', '調查', '辦理', '偵訊', '訊問', '諭令', '指揮', '提起', '指出', '表示', \n",
    "               '搜索', '認定', '認為', '獲報', '接獲', '依據', '報告', '拿出', '負責']\n",
    "    if re.search(f'(檢察官|員警|警察|律師|監委|廳長)(...、)?{name}(、...)?則?({\"|\".join(da_list)})', sentence):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# create target's features\n",
    "def create_sentence_list(test_news:str, name_list:list) -> list:\n",
    "    \"\"\"@test_news: 原文\n",
    "       @name_list: ner抓出的人名\n",
    "       @return: sentence_list, 用於下一階段      \n",
    "    \"\"\"\n",
    "    sentence_list = []\n",
    "    innocent_list = []\n",
    "    # 切分句子\n",
    "    test_split_content = test_news\n",
    "    test_split_content = test_split_content.replace('。','=。').replace('，','*，').replace('？','+？').replace('；','{；')\n",
    "    test_split_content = re.split('，|。|？|；', test_split_content)\n",
    "    test_split_content = list(map(lambda x: x.replace('=','。').replace('*','，').replace('+','？').replace('{','；'), test_split_content))\n",
    "    test_split_content = list(filter(None, test_split_content))\n",
    "    # 原人名list排序 \n",
    "    name_list.sort(key=lambda x: len(x), reverse=True)\n",
    "    for name in name_list:\n",
    "        tmp_name_list = name_list.copy()\n",
    "        tmp_name_list.remove(name)\n",
    "        full_name_list = tmp_name_list.copy()\n",
    "        tmp_name_list = [n for n in tmp_name_list if ((len(n) < 3) & (n[0] != name[0])) | (len(n) > 2)]\n",
    "        tmp_name_list = [n.replace('?', '\\?') for n in tmp_name_list]\n",
    "        # 建立sentences list, 即上拼接下文\n",
    "        for i, s in enumerate(test_split_content):\n",
    "            sentence = ''\n",
    "            if name in s:\n",
    "                # 正常有前後文共3句的情況\n",
    "                # 第1句出現句號 -> 拼2+3\n",
    "                # 第2句出現句號 -> 拼1+2\n",
    "                # 第3句或沒有句號 -> 拚1+2+3\n",
    "                # 若名字出現在頭尾的例外處理                      \n",
    "                try:\n",
    "                    if i != 0:\n",
    "                        start = test_split_content[i-1]\n",
    "                        mid = test_split_content[i]\n",
    "                        end = test_split_content[i+1]\n",
    "                        \n",
    "                        if start.find('。') > 0:\n",
    "                            start = ''\n",
    "                        if mid.find('。') > 0:\n",
    "                            end = ''\n",
    "                        if sum([1 for n in full_name_list if n in start]) > 0:\n",
    "                            start = ''\n",
    "                        if sum([1 for n in full_name_list if n in end]) > 0:\n",
    "                            end = ''\n",
    "                        mid = re.sub('|'.join(tmp_name_list), '', mid)            \n",
    "                        sentence = start + mid + end     \n",
    "                    else:\n",
    "                        mid = test_split_content[i]\n",
    "                        end = test_split_content[i+1]\n",
    "                        if mid.find('。') > 0:\n",
    "                            end = ''\n",
    "                        if sum([1 for n in full_name_list if n in end]) > 0:\n",
    "                            end = ''\n",
    "                        mid = re.sub('|'.join(tmp_name_list), '', mid)      \n",
    "                        sentence = mid + end\n",
    "                except:\n",
    "                    start = test_split_content[i-1] \n",
    "                    mid = test_split_content[i]\n",
    "                    if start.find('。') > 0:\n",
    "                        start = ''\n",
    "                    if sum([1 for n in full_name_list if n in start]) > 0:\n",
    "                        start = ''\n",
    "                    mid = re.sub('|'.join(tmp_name_list), '', mid) \n",
    "                    sentence = start + mid           \n",
    "                sentence_list.append((sentence, name))\n",
    "                # 無罪規則\n",
    "                if (('無罪定讞' in mid) | ('確定無罪' in mid) | ('無罪確定' in mid)| ('罪嫌不足' in mid) | ('罪證不足' in mid) | ('不起訴' in mid)) & ('、' not in mid):\n",
    "                    innocent_list.append(name)\n",
    "                try:\n",
    "                    if (('無罪定讞' in end) | ('確定無罪' in end) | ('無罪確定' in end)| ('罪嫌不足' in end) | ('罪證不足' in end) | ('不起訴' in end)) & ('、' not in end):\n",
    "                        innocent_list.append(name)\n",
    "                except:\n",
    "                    pass\n",
    "                # 檢察官規則\n",
    "                try:\n",
    "                    if search_da(name, mid+end):\n",
    "                        innocent_list.append(name)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    return sentence_list, innocent_list\n",
    "\n",
    "# create innocent people list \n",
    "def innocent_list_patch(innocent_list:list, name_list:list):\n",
    "\n",
    "    innocent_name_list = []\n",
    "    full_name = innocent_list.copy()\n",
    "    full_3name = list(set([name for name in name_list if len(name) == 3]))    \n",
    "    a = Counter([name[0] for name in full_3name])\n",
    "    keep = [k for k,v in a.items() if v == 1]\n",
    "    \n",
    "    full_3name_filter = [name for name in full_3name if name[0] in keep]\n",
    "    name_dict = dict((name[0], name) for name in full_3name_filter)   # ex: {'陳' : '陳水扁'}\n",
    "    for name in full_name:\n",
    "        if (name[0] in name_dict.keys()) & (len(name) == 1):\n",
    "            innocent_name_list.append(name_dict.get(name[0]))\n",
    "        else:\n",
    "            innocent_name_list.append(name)\n",
    "                \n",
    "    return innocent_name_list    \n",
    "\n",
    "# create dataset for model 2 \n",
    "def create_dataset(sentence_list:list) -> pd.DataFrame:\n",
    "    AML = pd.DataFrame(sentence_list, columns=['sentence', 'name'])     \n",
    "    name_list = []\n",
    "\n",
    "    #姓氏表\n",
    "    first_name = ['浦', '藺', '俄', '嵇', '莘', '吉', '廉', '空', '章', '金', '蔡', '謝', '邴', '任', '江', '雍', '宮', '洪', '範', '闕', '歐', '經', '溫', '吳', \n",
    "                  '匡', '巫', '薄', '尚', '武', '全', '龔', '陽', '蒲', '錢', '關', '戈', '慎', '朱', '施', '刁', '文', '養', '桑', '閆', '汝', '談', '能', '蓋', \n",
    "                  '毛', '厙', '白', '王', '郁', '屠', '東', '杜', '靳', '涂', '笪', '欒', '郎', '扶', '晏', '封', '倪', '艾', '冷', '於', '祿', '陳', '陶', '宦', \n",
    "                  '盧', '沈', '鄧', '聞', '翟', '都', '苗', '戎', '咸', '米', '弘', '池', '穆', '仲', '林', '童', '牛', '蓬', '何', '翁', '佘', '幸', '路', '蕭', \n",
    "                  '班', '李', '樊', '壽', '易', '支', '安', '費', '畢', '俞', '祖', '酆', '羿', '查', '牧', '齊', '孫', '車', '和', '庾', '黨', '別', '常', '邵', \n",
    "                  '有', '汲', '況', '薊', '丁', '皮', '鄔', '曹', '穀', '訾', '卻', '管', '鄢', '仇', '諸', '賈', '湯', '仰', '緱', '瞿', '嚴', '充', '燕', '甯', \n",
    "                  '索', '薛', '勾', '商', '廖', '申', '左', '韶', '阮', '璩', '谷', '黎', '陰', '晁', '相', '狄', '鞏', '從', '愛', '鮑', '夔', '寇', '汪', '利',\n",
    "                  '范', '彭', '暴', '逯', '惠', '蒼', '姬', '衛', '遊', '邱', '荊', '國', '蔣', '屈', '焦', '康', '豐', '于', '邢', '程', '周', '呂', '平', '帥', \n",
    "                  '亢', '郝', '凌', '喻', '鐘', '桓', '魯', '步', '盛', '貢', '松', '符', '賴', '芮', '佴', '秦', '越', '佟', '榮', '龐', '暨', '蒯', '楊', '農', \n",
    "                  '邰', '宋', '石', '伏', '塗', '危', '貝', '井', '賁', '習', '鈄', '昝', '秋', '奚', '溥', '解', '梅', '徐', '辛', '戚', '段', '乜', '葛', '沙', \n",
    "                  '湛', '麻', '馮', '柴', '烏', '席', '幹', '琴', '曾', '干', '賀', '勞', '趙', '韓', '曆', '滕', '郤', '譚', '糜', '沃', '年', '濮', '水', '顏', \n",
    "                  '滿', '劉', '虞', '茅', '莊', '蒙', '強', '敖', '顧', '方', '時', '山', '牟', '袁', '饒', '毋', '逄', '融', '單', '潘', '喬', '衡', '鞠', '宰', \n",
    "                  '鹹', '郟', '閔', '桂', '史', '董', '盍', '容', '卜', '元', '蔚', '尹', '霍', '黃', '蘇', '冉', '哈', '姜', '廣', '華', '梁', '熊', '逮', '欽', \n",
    "                  '隗', '郗', '胡', '殳', '闞', '家', '紀', '伊', '許', '雲', '邊', '終', '益', '昌', '居', '夏', '聶', '楚', '岳', '師', '竇', '司', '龍', '項', \n",
    "                  '羅', '甄', '古', '殷', '游', '法', '宓', '麴', '詹', '田', '連', '道', '高', '孔', '花', '馬', '姚', '杭', '簡', '宿', '甘', '柏', '鄭', '冀', \n",
    "                  '公', '岑', '儲', '滑', '后', '魚', '郜', '耿', '祁', '郭', '駱', '崔', '葉', '淩', '向', '巢', '弓', '裘', '宣', '鍾', '隆', '富', '雷', '宗', \n",
    "                  '侯', '臧', '通', '須', '堵', '晉', '竺', '顔', '印', '傅', '景', '權', '厲', '韋', '包', '樂', '繆', '荀', '成', '孟', '唐', '籍', '紅', '萬', \n",
    "                  '尤', '譙', '柯', '婁', '束', '茹', '房', '酈', '墨', '柳', '張', '裴', '雙', '鳳', '懷', '陸', '巴', '莫', '禹', '後', '扈', '羊', '藍', '祝', \n",
    "                  '蔔', '鄒', '計', '慕', '閻', '應', '余', '舒', '僪', '那', '伍', '鬱', '胥', '魏', '明', '戴', '餘', '卓', '褚', '鈕', '鄂', '季', '卞']\n",
    "\n",
    "    # name 拿出來\n",
    "    full_name = list(AML['name'])\n",
    "    # 三字人名取 unique\n",
    "    # full_3name = list(set([n for n in list(set(full_name)) if len(n) == 3]))\n",
    "    full_3name = list(set([n for n in list(set(full_name)) if len(n) in [3, 4]]))\n",
    "    full_longname = list(set([n for n in list(set(full_name)) if len(n) > 3]))\n",
    "\n",
    "    a = Counter([name[0] for name in full_3name])\n",
    "    keep = [k for k,v in a.items() if v == 1]\n",
    "    \n",
    "    name_dict_2 = dict(zip([name[0:2] for name in full_3name], full_3name))  # ex: {'王音': '王音之'}\n",
    "    name_dict_3 = dict(zip([name[1:] for name in full_longname], full_longname))  \n",
    "    name_dict_4 = dict(zip([name[:2] for name in full_longname], full_longname)) \n",
    "    for name in full_name:\n",
    "        if (name in name_dict_2.keys()) & (len(name) == 2):\n",
    "            name_list.append(name_dict_2.get(name))\n",
    "        elif (name in name_dict_3.keys()):\n",
    "            name_list.append(name_dict_3.get(name))\n",
    "        elif (name in name_dict_4.keys()) & (len(name) == 2):\n",
    "            name_list.append(name_dict_4.get(name))\n",
    "        else:\n",
    "            name_list.append(name)\n",
    "    \n",
    "    # 排除重複資料、排除一字、兩字簡稱、兩字三字四字姓不在姓氏表中的人\n",
    "    AML['name'] = name_list\n",
    "    AML = AML.drop_duplicates()\n",
    "    AML = AML[AML['name'].apply(lambda x: (len(x) > 1) )]\n",
    "    AML = AML[~AML['name'].apply(lambda x: (len(x) == 2) & (x[1] in ['男', '嫌', '婦', '夫', '某', '女', '妻', '員', '稱', '家', '哥', '媽', '生',  '揆', '董', '母', '公', '少', '翁', '粉', '仔', '氏', '父', '童', '弟', '嬤', '姐', '姊', '警']))]\n",
    "    AML = AML[~AML['name'].apply(lambda x: (len(x) == 2) & (x[0] in ['小', '阿', '老']))]\n",
    "    AML = AML[~AML['name'].apply(lambda x: (len(x) == 2) & (x[0] == x[1]))]\n",
    "    AML = AML[AML['name'].apply(lambda x: (len(x) > 2) | ((len(x) < 3) & (x[0] in first_name)))]\n",
    "    AML = AML[~AML['name'].apply(lambda x: (x[0] not in first_name) & (len(x) in (4,3,2)) )]\n",
    "    # AML = AML[~AML.apply(lambda x: search_da(x['name'], x['sentence']), axis=1)]\n",
    "\n",
    "    return AML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_aml(model, data, aml_threshold):\n",
    "    \n",
    "    #第一階段預測，大於aml_threshold者為疑似aml文章   \n",
    "    prediction = model.predict(data)\n",
    "    prediction[prediction >= aml_threshold] = 1\n",
    "    prediction[prediction < aml_threshold] = 0\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# 取得名字 (預測結果為onehot的狀態)\n",
    "def get_name(input_id, y_pred, token_dict):\n",
    "    \n",
    "    label_list = []\n",
    "    word_dict = {v: k for k, v in token_dict.items()}\n",
    "    \n",
    "    for input_data, y in zip(input_id, y_pred):\n",
    "        people_index = ''.join([str(a) for a in list(y)])\n",
    "        j = 0\n",
    "        name_list = []\n",
    "        split_index = re.findall('[12]2*', people_index)\n",
    "        name = ''.join([word_dict.get(input_data[index]) for index, value in enumerate(y) if value != 0])\n",
    "        \n",
    "        # [UNK], [PAD]會被算成 5 個字元，避免轉換成文字的index因長度不同對不上，故用 1 個字元的其他符號替代\n",
    "        # 王春甡 -> 王春[UNK] -> 王春?\n",
    "        name = name.replace('[UNK]','?')\n",
    "        name = name.replace('[PAD]','!')\n",
    "        \n",
    "        for i in split_index:\n",
    "            name_list.append(name[0+j:len(i)+j])\n",
    "            j = len(i) + j\n",
    "            \n",
    "        name_list = [name for name in name_list]\n",
    "        label_list.append(list(set(name_list)))\n",
    "    \n",
    "    return label_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = '記者楊政郡／台中報導?2014年間利鑫公司推出「F.A.S.Ttm基金」（未經許可及合法設立登記），由陳思哲引介「阮涵財」或「林玉婷」（真實姓名皆不詳），以非法多層次傳銷方式吸金，達615萬美金（約新台幣1億8450萬）及166萬港幣（約新台幣747萬），台灣負責人陳思哲依違反銀行法加重罪判8年6月徒刑。判決書指出，陳思哲明知利鑫外匯公司（瑞士商）未向我國申請許可及公司設立登記，非銀行機構，竟與自稱利鑫公司顧問之「阮涵財」或「林玉婷」等人共謀，自2014年元月起，由陳思哲對外招攬不特定人參與投資，在中市、高雄市、台北市、新竹市等地，租借飯店舉辦利鑫公司投資說明會，說明會中由陳思哲介紹，「阮涵財」或「林玉婷」向與會不特定民眾解說「F.A.S.Ttm基金」投資方案及獎金種類。誆稱所收取資金，將操作外匯投資和貨幣衍生品，前景可期，參與投資會員，投資額1萬至2萬9900美元範圍，每週可固定獲利2%（稱基本配套）；投資額為3萬至9萬9900美元範圍，每週可固定獲利3%（稱無限配套）；投資額為10萬至50萬美元範圍，每週可獲利3.1%至3.5%不等（稱鑫級配套）。會員招攬下線投資，成為會員，每週可領取第1層下線週分紅30%、第2層下線週分紅20%、第3層至第10層週分紅10%與第11層至第25層週分紅5%不等獎金，以此非法多層次傳銷方式，吸引不特人投入資金。陳思哲以上述方式陸續招約20名投資者，吸收資金共615萬餘美元（折新台幣1億8450萬）及港幣166萬（折新台幣747萬）餘元，同年10月利鑫公司未再支付各投資人紅利，始知受騙。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jason\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "\n",
    "tokenizer, token_dict = create_tokenizer(dict_path=dict_path)\n",
    "test_news = clean_marks(input_data)\n",
    "test_news_2 = rebuild_sentence(test_news, maxlen)\n",
    "data = encoded(tokenizer=tokenizer, data=test_news_2, maxlen=maxlen)\n",
    "\n",
    "# 第一階段預測，大於aml_threshold者為疑似aml文章   \n",
    "prediction_1 = model.predict(data)\n",
    "\n",
    "# 第一階段如果大於 aml threshold, 過model 1.5再判斷\n",
    "if prediction_1 >= aml_threshold:\n",
    "\n",
    "    data_2 = encoded(tokenizer=tokenizer, data=test_news_2, maxlen=maxlen_aml)\n",
    "    prediction1_5 = model1_5.predict(data_2) \n",
    "\n",
    "    # 階段1.5 > 0.3 則進入第二階段\n",
    "    if prediction1_5 >= 0.3:      \n",
    "        # phase 2 \n",
    "        # model ner: 用於提取文章中的人名    \n",
    "        try:\n",
    "            test_ner = split_content(test_news)\n",
    "            name_list = []              \n",
    "            for i in range(len(test_ner)):\n",
    "                input_id, segment_id, mask_input = encoded(tokenizer, test_ner[i], maxlen=maxlen_ner)\n",
    "\n",
    "                ner_prediction = ner_model.predict([input_id, segment_id, mask_input])\n",
    "                y_pred = np.argmax(ner_prediction, axis=-1)                  \n",
    "                tmp_list = get_name(input_id, y_pred, token_dict)[0]\n",
    "                name_list.extend(tmp_list)\n",
    "\n",
    "            # 處理拿到的人名\n",
    "            name_list = list(set(name_list))\n",
    "            name_list = ['' if len(re.findall('[()<>{}\\[\\]]', n)) > 0 else n for n in name_list]\n",
    "        except Exception as e:\n",
    "            prediction = []\n",
    "        # phase 3-1\n",
    "        # data cleansing & create dataset \n",
    "\n",
    "        # 清理人名, 依照原文補回字典沒有的字, \n",
    "        # 即將出現 ?或 !的人名補回原形\n",
    "        try:\n",
    "            for i, n in enumerate(name_list):\n",
    "                if ('?' in n) | ('!' in n):\n",
    "                    reexp = n.replace('?', '.').replace('!', '.')\n",
    "                    reexp = re.compile(reexp, re.IGNORECASE)\n",
    "                    name_list[i] = re.search(reexp, test_news).group() \n",
    "            name_list = list(set(name_list))  \n",
    "            # 取文章內出現人名的前後文        \n",
    "            sentence_list, innocent_list = create_sentence_list(test_news, name_list)   \n",
    "\n",
    "            # 填補 innocent_list\n",
    "            innocent_list = list(set(innocent_list))\n",
    "            innocent_list = innocent_list_patch(innocent_list, name_list)\n",
    "\n",
    "            # 準備dataset\n",
    "            aml_dataset = create_dataset(sentence_list)\n",
    "        except Exception as e:\n",
    "            prediction = []\n",
    "\n",
    "        # phase 3-2\n",
    "        # model 2: 預測該句子是否跟AML相關, 若相關, 則可依照閾值提取關鍵人名      \n",
    "        try:\n",
    "            input_id, segment_id, mask_input = encoded_2(tokenizer, aml_dataset, maxlen=maxlen_sentences)\n",
    "            prediction_2 = model2.predict([input_id, segment_id, mask_input])\n",
    "            aml_dataset['prediction'] = prediction_2\n",
    "            aml_dataset['prediction'][aml_dataset['name'].isin(innocent_list)] = 0\n",
    "            aml_dataset['prediction'] = aml_dataset['prediction'].apply(lambda x: 0 if x < threshold else 1)\n",
    "            aml_dataset = aml_dataset.groupby(['name'])['prediction'].max().reset_index()\n",
    "            aml_dataset = aml_dataset[aml_dataset['prediction'] == 1]\n",
    "            aml_name_list = aml_dataset['name'].values.tolist()\n",
    "            prediction = list(set(aml_name_list))\n",
    "        except Exception as e:\n",
    "            prediction = []\n",
    "    # 無關就直接回傳空list \n",
    "    else:\n",
    "        prediction = []\n",
    "else:\n",
    "    prediction = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['陳思哲']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
